## Notes
- API [documentation](https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md#api-endpoints).
- Qwen-LM is used a baseline. Config is imported from [here](https://qwen.readthedocs.io/en/latest/run_locally/llama.cpp.html#llama-server).
- llama.cpp requires `GGUF` model format. All Qwen models (not only `GGUF`) are listed [here](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f).
- [List](https://github.com/ggml-org/llama.cpp/tree/master?tab=readme-ov-file#text-only) of all models which are supported by llama.cpp.
